{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## genism\n",
    "# genism intro: https://radimrehurek.com/gensim/\n",
    "# anaconda-genism install: https://anaconda.org/anaconda/gensim\n",
    "\n",
    "## opencc\n",
    "# https://github.com/BYVoid/OpenCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST 1\n",
    "reference: http://www.jianshu.com/p/77d161cb0509  <br>\n",
    "Download => https://drive.google.com/file/d/1I2nXPBoHUcQ9D8HnQVDUYx_SGiRJPNXG/view?usp=sharing <br>\n",
    "Download => https://drive.google.com/open?id=1-EoeXkdde1LI6eY_sym8muRNXGinAUkc <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# 數據預處理\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
    "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
    "        ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop] # use stop_words\n",
    "    return tokenized\n",
    "\n",
    "# 把原始的TXT整理成為CSV\n",
    "# basepath = 'aclImdb'\n",
    "# labels = {'pos': 1, 'neg': 0}\n",
    "# pbar = pyprind.ProgBar(50000)\n",
    "# df = pd.DataFrame()\n",
    "# for s in ('test', 'train'):\n",
    "#     for l in ('pos', 'neg'):\n",
    "#         path = os.path.join(basepath, s, l)\n",
    "#         for file in os.listdir(path):\n",
    "#             with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
    "#                 txt = infile.read()\n",
    "#                 token = tokenizer(text=txt)\n",
    "#             df = df.append([[token, labels[l]]], ignore_index=True)\n",
    "#             pbar.update()\n",
    "# df.columns = ['review', 'sentiment']\n",
    "# np.random.seed(0)\n",
    "# df = df.reindex(np.random.permutation(df.index))\n",
    "# df.to_csv('movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成基於此數據集的word2vec模型\n",
    "import gensim.models\n",
    "\n",
    "class csvStream(object):\n",
    "    def __init__(self,path):\n",
    "        self.path=path\n",
    "    def __iter__(self):\n",
    "        with open(self.path, 'r',) as csv:\n",
    "            next(csv)  # skip header\n",
    "            for line in csv:\n",
    "                text = line[4:-3]\n",
    "                text = re.sub('[\\'\\\"\\[\\]\\d\\b]','',text)   \n",
    "                while (text[0] == ',') or (text[0] == ' '):\n",
    "                    text = text[1:]\n",
    "                pbar.update()\n",
    "                yield text.split(', ')\n",
    "\n",
    "# inpath = 'movie_data.csv'\n",
    "# outpath = 'wordVectTrainResult'\n",
    "# pbar = pyprind.ProgBar(100000)\n",
    "\n",
    "# lineIterator = csvStream(inpath)\n",
    "# model = gensim.models.Word2Vec()\n",
    "# model.build_vocab(lineIterator)\n",
    "# print('vocabulary building finished, start training...')\n",
    "# model.train(lineIterator,total_examples=model.corpus_count,epochs=1)\n",
    "# model.save(outpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the trained word2vec model\n",
    "inpath = 'wordVectTrainResult'\n",
    "model = gensim.models.Word2Vec.load(inpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('father', 0.9493988156318665),\n",
       " ('husband', 0.9406527280807495),\n",
       " ('finds', 0.9374130368232727),\n",
       " ('sister', 0.9350247383117676),\n",
       " ('daughter', 0.9319608211517334)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get most similar\n",
    "model.most_similar(positive=['woman', 'mother'], negative=['man'], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92745650756960141"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check similarity\n",
    "model.similarity('woman', 'man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('girl', 0.9343360662460327),\n",
       " ('mother', 0.9312624335289001),\n",
       " ('man', 0.9274565577507019),\n",
       " ('daughter', 0.9190661907196045),\n",
       " ('wife', 0.9163395166397095),\n",
       " ('father', 0.9118176698684692),\n",
       " ('sister', 0.9084187746047974),\n",
       " ('husband', 0.9028269052505493),\n",
       " ('brother', 0.9000281691551208),\n",
       " ('son', 0.899724006652832)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check one word's similarity top 10\n",
    "model.similar_by_word('woman')\n",
    "# model.most_similar(positive=['woman'], topn=10)   # the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('film', 0.88144850730896),\n",
       " ('sequel', 0.8534958362579346),\n",
       " ('succubare', 0.8423082232475281),\n",
       " ('ones', 0.8210549354553223),\n",
       " ('spacecamp', 0.8202271461486816),\n",
       " ('comment', 0.8188279271125793),\n",
       " ('flick', 0.8168118000030518),\n",
       " ('thismovie', 0.8158571720123291),\n",
       " ('definitely', 0.8152419328689575),\n",
       " ('painful', 0.8151455521583557)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similar_by_word('movie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST 2\n",
    "reference: https://rare-technologies.com/word2vec-tutorial/  <br>\n",
    "reference: http://www.jianshu.com/p/52ee8c5739b6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST 3\n",
    "reference: http://zake7749.github.io/2016/08/28/word2vec-with-gensim/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1 檔案下載 與 處理\n",
    "    # 1.1 預先下載 wiki 數據庫`(https://zh.wikipedia.org/wiki/Wikipedia:%E6%95%B0%E6%8D%AE%E5%BA%93%E4%B8%8B%E8%BD%BD)\n",
    "    # 2.2 使用 genism內建之wikiCorpus，將下載之zhwiki-20171201-pages-articles.xml.bz2進行xml解析處理。\n",
    "    # 1.3 存成檔案 wiki_texts.txt。\n",
    "# => 已處理過，約3X萬筆 https://drive.google.com/file/d/1ulB2uoyDKdSxtrCs8sc7SEIyjGMruumz/view?usp=sharing "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
