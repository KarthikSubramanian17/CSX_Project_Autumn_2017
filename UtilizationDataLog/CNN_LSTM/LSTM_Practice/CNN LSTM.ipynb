{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN LSTM\n",
    "A Convolutional Neural Network is used to learn features in spatial input and the LSTM is used to support a sequence of inputs (e.g. video of images).\n",
    "- Gentle introduction to CNN LSTM recurrent neural networks with example Python code.\n",
    "- Book https://machinelearningmastery.com/cnn-long-short-term-memory-networks/\n",
    "- Book https://machinelearningmastery.com/lstms-with-python/\n",
    "- Paper https://arxiv.org/pdf/1411.4389v4.pdf\n",
    "- https://stackoverflow.com/questions/44778439/keras-tf-time-distributed-cnnlstm-for-visual-recognition\n",
    "\n",
    "Problem Issue\n",
    "- https://github.com/fchollet/keras/issues/5527\n",
    "- https://github.com/fchollet/keras/issues/401\n",
    "- https://github.com/fchollet/keras/issues/4172\n",
    "\n",
    "<h3 style=\"background-color:#a2b5fb\"> CNN LSTM Architecture </h3>\n",
    "\n",
    "#### The CNN LSTM architecture involves \n",
    "- using **Convolutional Neural Network (CNN) layers** for **feature extraction on input data** \n",
    "- combined with **LSTMs** to **support sequence prediction**.\n",
    "\n",
    "#### CNN LSTMs were developed for \n",
    "- **visual time series prediction problems** \n",
    "- **generating textual descriptions** from sequences of images (e.g. videos). \n",
    "\n",
    "#### Specifically, the problems of:\n",
    "- **Activity Recognition**: Generating a textual description of an activity demonstrated in a sequence of images.\n",
    "- **Image Description**: Generating a textual description of a single image.\n",
    "- **Video Description**: Generating a textual description of a sequence of images.\n",
    "\n",
    "#### This architecture was **originally referred** to as \n",
    "- a Long-term Recurrent Convolutional Network or LRCN model,\n",
    "- will use the more generic name “CNN LSTM” to refer to LSTMs that use a CNN as a front end in this lesson.\n",
    "\n",
    "#### This architecture is used for \n",
    "- the task of generating textual descriptions of images. \n",
    "- Key is the use of a CNN that is pre-trained on a challenging image classification task that is re-purposed as a feature extractor for the caption generating problem\n",
    "-  it is natural to use a CNN as an image “encoder”, by first pre-training it for an image classification task and using the last hidden layer as an input to the RNN decoder that generates sentences( Show and Tell: A Neural Image Caption Generator, 2015.)\n",
    "- This architecture has **also been used** on **speech recognition** and n**atural language processing** problems where **CNNs are used as feature extractors for the LSTMs on audio and textual input data.**\n",
    "\n",
    "#### This architecture is appropriate for problems that:\n",
    "- Have **spatial structure in their input** such as the **2D structure or pixels in an image** or the **1D structure of words in a sentence, paragraph, or document.**\n",
    "- Have a **temporal structure in their input** such as the **order of images in a video or words in text**, or require **the generation of output with temporal structure such as words in a textual description.**\n",
    "\n",
    "<h3 style=\"background-color:#a2b5fb\"> Implement CNN LSTM in Keras </h3>\n",
    "#### We can define a CNN LSTM model to be trained jointly in Keras.\n",
    "- A CNN LSTM can be defined by adding CNN layers on the front end followed by LSTM layers with a Dense layer on the output.\n",
    "\n",
    "#### It is helpful to think of this architecture as defining two sub-models: \n",
    "- the **CNN Model** for **feature extraction**. \n",
    "- the **LSTM Model** for **interpreting the features across time steps.**\n",
    "\n",
    "<img style=\"float:left;height:250px;width:auto;margin-right:30px\" src=\"CNN_LSTM.png\">\n",
    "#### define CNN model\n",
    "<pre>\n",
    "model = Sequential()\n",
    "model.add(TimeDistributed(Conv2D(...))\n",
    "model.add(TimeDistributed(MaxPooling2D(...)))\n",
    "model.add(TimeDistributed(Flatten()))\n",
    "</pre>\n",
    "\n",
    "#### define LSTM model\n",
    "<pre>\n",
    "model.add(LSTM(...))\n",
    "model.add(Dense(...))\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color:#a2b5fb\"> 資料繪圖 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_image(image):\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(2,2)\n",
    "    plt.imshow(image, cmap='binary')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_images_labels_prediction(images, labels, \n",
    "                                  prediction, idx, num=10): #(影像, 真實值, 預測結果, 資料開始index, 顯示筆數)\n",
    "    fig = plt.gcf() #圖初始\n",
    "    fig.set_size_inches(12, 14) #圖大小\n",
    "    if num>25: num=25 #筆數限制\n",
    "    for i in range(0, num):  \n",
    "        ax = plt.subplot(5, 5, 1+i) #subgraph大小，位置(5行, 5列, 1開始位置)\n",
    "        ax.imshow(images[idx], cmap='binary') #畫出subgraph\n",
    "        title= \"lable=\" + str(labels[idx]) #subgraph title\n",
    "        if len(prediction)>0:\n",
    "            title+=\",prediction=\"+str(prediction[idx]) #subgraph title with prediction\n",
    "        ax.set_title(title, fontsize=10)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        idx +=1\n",
    "    plt.show()\n",
    "\n",
    "def show_train_history(train_histroy, train, validation):\n",
    "    plt.plot(train_history.history[train])\n",
    "    plt.plot(train_history.history[validation])\n",
    "    plt.title('Train History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(train)\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN LSTM Demo\n",
    "<h3 style=\"background-color:#a2b5fb\"> 資料預處理 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 讀入模組與資料集\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(10)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 讀取資料,切為(x_Train, y_Train), (x_Test, y_Test)\n",
    "(x_Train, y_Train), (x_Test, y_Test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 特徵轉4維矩陣, 並標準化以提高準確度及收斂速度\n",
    "x_Train4D=x_Train.reshape(x_Train.shape[0], 28, 28, 1).astype('float32') #(60000, 28, 28, 1)\n",
    "x_Test4D=x_Test.reshape(x_Test.shape[0], 28, 28, 1).astype('float32') #(60000, 28, 28, 1)\n",
    "x_Train4D_normalize = x_Train4D/255\n",
    "x_Test4D_normalize = x_Test4D/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 標籤Category轉OneHot\n",
    "y_TrainOneHot=np_utils.to_categorical(y_Train)\n",
    "y_TestOneHot=np_utils.to_categorical(y_Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color:#a2b5fb\"> 建立模型 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 讀入模組\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, TimeDistributed,Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_cnn = Sequential()\n",
    "# Conv2D\n",
    "model_cnn.add(Conv2D(filters=16, kernel_size=(5,5), padding='same', input_shape=(28,28,1), activation='relu'))\n",
    "# MaxPooling2D \n",
    "model_cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# Conv2D\n",
    "model_cnn.add(Conv2D(filters=36, kernel_size=(5,5), padding='same', activation='relu'))\n",
    "# MaxPooling2D \n",
    "model_cnn.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# DropOut\n",
    "model_cnn.add(Dropout(0.25))\n",
    "# Flatten\n",
    "model_cnn.add(Flatten())\n",
    "# Dense\n",
    "model_cnn.add(Dense(units=128, activation='relu'))\n",
    "# DropOut\n",
    "model_cnn.add(Dropout(0.5))\n",
    "# Dense\n",
    "model_cnn.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "print(model_cnn.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# https://groups.google.com/forum/#!topic/keras-users/glng2f67Hfs\n",
    "model_CL = Sequential()\n",
    "model_CL.add(TimeDistributed(Conv2D(filters=16, kernel_size=(5,5), padding='same', activation='relu'),\n",
    "                             input_shape = (2, 28, 28, 1)))\n",
    "model_CL.add(TimeDistributed((MaxPooling2D(pool_size=(2,2)))))\n",
    "model_CL.add(TimeDistributed(Conv2D(filters=36, kernel_size=(5,5), padding='same', activation='relu')))\n",
    "model_CL.add(TimeDistributed((MaxPooling2D(pool_size=(2,2)))))\n",
    "model_CL.add(TimeDistributed(Dropout(0.25)))\n",
    "model_CL.add(TimeDistributed(Flatten()))\n",
    "model_CL.add(LSTM(32, activation='sigmoid'))\n",
    "model_CL.add(Dense(units=128, activation='relu'))\n",
    "model_CL.add(Dropout(0.5))\n",
    "model_CL.add(Dense(units=10, activation='softmax'))\n",
    "\n",
    "print(model_CL.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"background-color:#a2b5fb\"> 訓練模型 </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compile model_cnn\n",
    "model_cnn.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training model_cnn\n",
    "train_history_cnn = model_cnn.fit(x=x_Train4D_normalize, \n",
    "                                  y=y_TrainOneHot,\n",
    "                                  validation_split=0.2,\n",
    "                                  epochs=10, \n",
    "                                  batch_size=300,\n",
    "                                  verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compile model_CL\n",
    "model_CL.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Training model_CL\n",
    "train_history_CL = model_CL.fit(x=x_Train4D_normalize, \n",
    "                                y=y_TrainOneHot,\n",
    "                                validation_split=0.2,\n",
    "                                epochs=10, \n",
    "                                batch_size=300,\n",
    "                                verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
